<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<style type="text/css">
body {font-family: "Avenir Book", Arial, sans-serif; font-weight: 300; font-size: 16px; margin: auto; width: 960px;}
h1, h2 {font-weight: 300;}
p {font-weight: 300; line-height: 1.4;}
code {font-size: 0.8rem; background: #efefef; padding: 0.5rem 0.8rem; border: 1px solid #d3d3d3; border-radius: 3px;}
img.rounded {border-radius: 10px;}
a:link, a:visited {color: #1367a7; text-decoration: none;}
</style>

<title>Provable Model-Parallel Distributed PCA with Parallel Deflation</title>
<meta property="og:title" content="Provable Model-Parallel Distributed PCA with Parallel Deflation" />
<meta property="og:description" content="A new distributed PCA framework leveraging parallel deflation." />

<body>
<br>
<center>
    <span style="font-size:32px">Provable Model-Parallel Distributed PCA with Parallel Deflation</span><br><br><br>
</center>

<table align="center" width="960px">
    <tr>
        <td align="center"><span style="font-size:16px"><a href="mailto:fangshuo.liao@rice.edu">Fangshuo Liao</a></span></td>
        <td align="center"><span style="font-size:16px"><a href="mailto:bs82@rice.edu">Wenyi Su</a></span></td>
        <td align="center"><span style="font-size:16px"><a href="mailto:anastasios@rice.edu">Anastasios Kyrillidis</a></span></td>
    </tr>
</table>

<table align="center" width="700px">
    <tr>
        <td align="center" colspan="2"><span style="font-size:16px">Computer Science Department, Rice University</span></td>
    </tr>
</table>

<br>
<table align="center" width="700px">
    <tr>
        <td align="center"><span style="font-size:20px">Code <a href="https://github.com/ParallelDeflation">[GitHub]</a></span></td>
        <td align="center"><span style="font-size:20px">Paper <a href="https://arxiv.org/pdf/parallel_deflation">[arXiv]</a></span></td>
    </tr>
</table>

<br>
<center>
    <h2>Abstract</h2>
</center>
<p>
Principal Component Analysis (PCA) is a fundamental tool in machine learning and data analysis. We propose a distributed PCA framework that enables multiple workers to compute distinct eigenvectors simultaneously using a novel <strong>Parallel Deflation Algorithm</strong>. Our method allows for asynchronous updates while maintaining provable convergence, addressing a key theoretical gap in model-parallel distributed PCA. We demonstrate that our approach achieves comparable performance to EigenGame-$\mu$, the state-of-the-art PCA solver, while providing stronger theoretical guarantees.
</p>

<br><hr>
<center>
    <h2>Introduction</h2>
</center>
<p>
PCA is widely used in dimensionality reduction and feature extraction. Traditional centralized approaches struggle with scalability in large datasets, necessitating distributed methods. Model-parallel PCA algorithms, such as EigenGame, have demonstrated success in distributing computations across multiple workers, but they rely on strict sequential dependencies.
</p>
<p>
We introduce <code>Parallel Deflation</code>, a model-parallel PCA approach that breaks the strict dependencies of previous methods, allowing multiple workers to refine their eigenvector estimates asynchronously. Our theoretical analysis provides provable convergence guarantees, a missing piece in prior model-parallel approaches.
</p>

<br><hr>
<center>
    <h2>Related Work</h2>
</center>
<p>
Several approaches have been explored for distributed PCA:
</p>
<ul>
    <li><strong>Eigenvector-based methods:</strong> Classical PCA relies on matrix decomposition techniques such as QR decomposition and Lanczos methods, which do not scale efficiently to distributed settings.</li>
    <li><strong>Stochastic optimization:</strong> Methods like Oja's rule and stochastic gradient approaches offer scalable alternatives but lack strong theoretical guarantees in distributed settings.</li>
    <li><strong>Data-parallel PCA:</strong> Existing distributed PCA frameworks partition data across workers but require centralized coordination, limiting efficiency.</li>
    <li><strong>Model-parallel PCA (EigenGame):</strong> Recent approaches model PCA as a game where workers compute individual components. However, existing methods require strict sequential dependencies and lack theoretical analysis.</li>
</ul>
<p>
Our work advances model-parallel PCA by removing strict sequential dependencies and providing provable convergence results.
</p>

<br><hr>
<center>
    <h2>Problem Statement</h2>
</center>
<p>
We focus on computing the top-K eigenvectors of an empirical covariance matrix <code>Σ</code>. Given a dataset <code>Y</code> with n samples and d features, the covariance matrix is given by:
</p>
<pre><code>Σ = Y^T Y</code></pre>
<p>
The goal is to find the top-K eigenvectors of <code>Σ</code> efficiently in a distributed setting, without requiring strict sequential dependencies.
</p>

<br><hr>
<center>
    <h2>The Parallel Deflation Algorithm</h2>
</center>
<p>
Our algorithm distributes the computation of K principal components across K workers. Unlike traditional deflation methods that require sequential eigenvector computation, we introduce a parallel iterative approach:
</p>
<ul>
    <li><strong>Initialization:</strong> Each worker starts with a rough estimate of its assigned eigenvector.</li>
    <li><strong>Parallel Updates:</strong> Workers refine their estimates iteratively, incorporating updates from other workers.</li>
    <li><strong>Deflation:</strong> The covariance matrix is deflated iteratively using the best available eigenvector approximations.</li>
</ul>
<img class="rounded" src="https://placeholder.com/parallel_deflation_diagram" alt="Parallel Deflation Algorithm" style="width:720px;">

<br><hr>
<center>
    <h2>Theory</h2>
</center>
<p>
We provide theoretical guarantees for the convergence of the Parallel Deflation Algorithm. Our analysis shows that:
</p>
<ul>
    <li><strong>Global Convergence:</strong> The algorithm converges to the true eigenvectors under mild assumptions.</li>
    <li><strong>Asynchronous Updates:</strong> The approach remains stable even when workers update asynchronously.</li>
    <li><strong>Error Bounds:</strong> We derive tight error bounds, demonstrating that convergence accelerates as updates propagate.</li>
</ul>
<p>
These results establish a strong theoretical foundation for model-parallel PCA, addressing a key gap in prior work.
</p>

<br><hr>
<center>
    <h2>Experiments</h2>
</center>
<p>
We compare <code>Parallel Deflation</code> against EigenGame-$\alpha$ and EigenGame-$\mu$ on synthetic datasets and real-world benchmarks such as MNIST and ImageNet.
</p>
<ul>
    <li><strong>Speed:</strong> Our method achieves faster convergence than EigenGame-$\alpha$ while matching EigenGame-$\mu$.</li>
    <li><strong>Scalability:</strong> Our approach scales efficiently to large datasets without requiring strict dependencies.</li>
    <li><strong>Accuracy:</strong> Achieves comparable reconstruction error to EigenGame-$\mu$ while offering theoretical guarantees.</li>
</ul>
<img class="rounded" src="https://placeholder.com/experiment_results" alt="Experimental Results" style="width:720px;">

<br><hr>
<center>
    <h2>Conclusion and Future Work</h2>
</center>
<p>
We present <code>Parallel Deflation</code>, a novel model-parallel PCA framework that removes sequential dependencies and provides provable convergence guarantees. Future work includes:
</p>
<ul>
    <li>Extending the method to other eigenvalue problems.</li>
    <li>Exploring adaptive update strategies for further efficiency.</li>
    <li>Applying the approach to large-scale deep learning models.</li>
</ul>

<br><hr>
<center>
    <h1>Acknowledgements</h1>
    <p>Supported by the Ken Kennedy Institute at Rice University.</p>
</center>
