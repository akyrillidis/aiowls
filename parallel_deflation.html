<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <style type="text/css">
        @font-face {
            font-family: 'Avenir Book';
            src: url("./fonts/Avenir_Book.ttf");
        }
        body {
            font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
            font-weight: 300;
            font-size: 16px;
            margin-left: auto;
            margin-right: auto;
            width: 960px;
        }
        h1, h2 {
            font-weight: 300;
        }
        p {
            font-weight: 300;
            line-height: 1.4;
        }
        code {
            font-size: 0.8rem;
            margin: 0 0.2rem;
            padding: 0.5rem 0.8rem;
            white-space: nowrap;
            background: #efefef;
            border: 1px solid #d3d3d3;
            color: #000000;
            border-radius: 3px;
        }
        pre > code {
            display: block;
            white-space: pre;
            line-height: 1.5;
            padding: 0;
            margin: 0;
        }
        pre.prettyprint > code {
            border: none;
        }
        .disclaimerbox {
            background-color: #eee;
            border: 1px solid #eeeeee;
            border-radius: 10px;
            padding: 20px;
        }
        video.header-vid, img.header-img {
            height: 140px;
            border: 1px solid black;
            border-radius: 10px;
        }
        img.rounded {
            border: 0px solid #eeeeee;
            border-radius: 10px;
        }
        a:link, a:visited {
            color: #1367a7;
            text-decoration: none;
        }
        a:hover {
            color: #208799;
        }
        td.dl-link {
            height: 160px;
            text-align: center;
            font-size: 22px;
        }
        .layered-paper-big {
            box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35),
                5px 5px 0 0px #fff,
                5px 5px 1px 1px rgba(0,0,0,0.35),
                10px 10px 0 0px #fff,
                10px 10px 1px 1px rgba(0,0,0,0.35),
                15px 15px 0 0px #fff,
                15px 15px 1px 1px rgba(0,0,0,0.35),
                20px 20px 0 0px #fff,
                20px 20px 1px 1px rgba(0,0,0,0.35),
                25px 25px 0 0px #fff,
                25px 25px 1px 1px rgba(0,0,0,0.35);
            margin-left: 10px;
            margin-right: 45px;
        }
        .layered-paper {
            box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35),
                5px 5px 0 0px #fff,
                5px 5px 1px 1px rgba(0,0,0,0.35),
                10px 10px 0 0px #fff,
                10px 10px 1px 1px rgba(0,0,0,0.35);
            margin-top: 5px;
            margin-left: 10px;
            margin-right: 30px;
            margin-bottom: 5px;
        }
        .vert-cent {
            position: relative;
            top: 50%;
            transform: translateY(-50%);
        }
        hr {
            border: 0;
            height: 1px;
            background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
        }
    </style>

    <title>Provable Model-Parallel Distributed PCA with Parallel Deflation</title>
    <meta property="og:title" content="Provable Model-Parallel Distributed PCA with Parallel Deflation" />
    <meta property="og:description" content="A new distributed PCA framework leveraging parallel deflation." />

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-75863369-6"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-75863369-6');
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>
<br>
<center>
    <span style="font-size:32px">Provable Model-Parallel Distributed PCA with Parallel Deflation</span><br><br><br>
</center>

<table align="center" width="960px">
    <tr>
        <td align="center"><span style="font-size:16px"><a href="mailto:fangshuo.liao@rice.edu">Fangshuo Liao</a></span></td>
        <td align="center"><span style="font-size:16px"><a href="mailto:bs82@rice.edu">Wenyi Su</a></span></td>
        <td align="center"><span style="font-size:16px"><a href="mailto:anastasios@rice.edu">Anastasios Kyrillidis</a></span></td>
    </tr>
</table>

<table align="center" width="960px">
    <tr>
        <td align="center" colspan="2"><span style="font-size:16px">Computer Science Department, Rice University and Ken Kennedy Institute (K2I)</span></td>
    </tr>
</table>

<br>
<table align="center" width="700px">
    <tr>
        <td align="center"><span style="font-size:20px">Code <a href="https://github.com/ParallelDeflation">[GitHub]</a></span></td>
        <td align="center"><span style="font-size:20px">Paper <a href="https://arxiv.org/pdf/2502.17615">[arXiv]</a></span></td>
    </tr>
</table>

<br>
<center>
    <h2>Abstract</h2>
</center>
<p>
Principal Component Analysis (PCA) is a fundamental tool in machine learning and data analysis. We propose a distributed PCA framework that enables multiple workers to compute distinct eigenvectors simultaneously using a novel <strong>Parallel Deflation Algorithm</strong>. Our method allows for asynchronous updates while maintaining provable convergence, addressing a key theoretical gap in model-parallel distributed PCA. We demonstrate that our approach achieves comparable performance to EigenGame-\(\mu\), the state-of-the-art PCA solver, while providing stronger theoretical guarantees.
</p>

<br><hr>
<center>
    <h2>Introduction</h2>
</center>
<p>
PCA is widely used in dimensionality reduction and feature extraction. Traditional centralized approaches struggle with scalability in large datasets, necessitating distributed methods. Model-parallel PCA algorithms, such as EigenGame, have demonstrated success in distributing computations across multiple workers, but they rely on strict sequential dependencies.
</p>
<p>
We introduce <code>Parallel Deflation</code>, a model-parallel PCA approach that breaks the strict dependencies of previous methods, allowing multiple workers to refine their eigenvector estimates asynchronously. Our theoretical analysis provides provable convergence guarantees, a missing piece in prior model-parallel approaches.
</p>

<br><hr>
<center>
    <h2>Related Work</h2>
</center>
<p>
Several approaches have been explored for distributed PCA:
</p>
<ul>
    <li><strong>Eigenvector-based methods:</strong> Classical PCA relies on matrix decomposition techniques such as QR decomposition and Lanczos methods, which do not scale efficiently to distributed settings.</li>
    <li><strong>Stochastic optimization:</strong> Methods like Oja's rule and stochastic gradient approaches offer scalable alternatives but lack strong theoretical guarantees in distributed settings.</li>
    <li><strong>Data-parallel PCA:</strong> Existing distributed PCA frameworks partition data across workers but require centralized coordination, limiting efficiency.</li>
    <li><strong>Model-parallel PCA (EigenGame):</strong> Recent approaches model PCA as a game where workers compute individual components. However, existing methods require strict sequential dependencies and lack theoretical analysis.</li>
</ul>
<p>
Our work advances model-parallel PCA by removing strict sequential dependencies and providing provable convergence results.
</p>

<br><hr>
<center>
    <h2>Problem Statement</h2>
</center>
<p>
We focus on computing the top-\(K\) eigenvectors of an empirical covariance matrix \(\Sigma\). Given a dataset \(Y\) with \(n\) samples and \(d\) features, the covariance matrix is given by:
\(\Sigma = Y^\top Y\). The goal is to find the top-\(K\) eigenvectors of \(\Sigma\) efficiently in a distributed setting, without requiring strict sequential dependencies.
</p>

<br><hr>
<center>
    <h2>The Parallel Deflation Algorithm</h2>
</center>
<p>
Our algorithm distributes the computation of \(K\) principal components across \(K\) workers. Unlike traditional deflation methods that require sequential eigenvector computation, we introduce a parallel iterative approach:
</p>
<ul>
    <li><strong>Initialization:</strong> Each worker starts with a rough estimate of its assigned eigenvector.</li>
    <li><strong>Parallel Updates:</strong> Workers refine their estimates iteratively, incorporating updates from other workers.</li>
    <li><strong>Deflation:</strong> The covariance matrix is deflated iteratively using the best available eigenvector approximations.</li>
</ul>
<center>
<img class="rounded" src="./assets/img/ParallelDeflationDiagram.png" alt="Parallel Deflation Algorithm" style="width:600px;">
<img class="rounded" src="./assets/img/pd_algorithm.png" alt="Parallel Deflation Algorithm" style="width:600px;">
</center>
<br><hr>
<center>
    <h2>Theory</h2>
</center>
<p>
We provide theoretical guarantees for the convergence of the Parallel Deflation Algorithm. Our analysis shows that:
</p>
<ul>
    <li><strong>Global Convergence:</strong> The algorithm converges to the true eigenvectors under mild assumptions.</li>
    <li><strong>Asynchronous Updates:</strong> The approach remains stable even when workers update asynchronously.</li>
    <li><strong>Error Bounds:</strong> We derive tight error bounds, demonstrating that convergence accelerates as updates propagate.</li>
</ul>
<p>
These results establish a strong theoretical foundation for model-parallel PCA, addressing a key gap in prior work.
</p>
<center>
<img class="rounded" src="./assets/img/pd_theory.png" alt="Parallel Deflation Algorithm" style="width:720px;">
</center>

<br><hr>
<center>
    <h2>Experiments</h2>
</center>
<p>
We compare <code>Parallel Deflation</code> against EigenGame-\(\alpha\) and EigenGame-\(\mu\) on synthetic datasets and real-world benchmarks such as MNIST and ImageNet.
</p>
<ul>
    <li><strong>Speed:</strong> Our method achieves faster convergence than EigenGame-\(\alpha\) while matching EigenGame-\(\mu\).</li>
    <li><strong>Scalability:</strong> Our approach scales efficiently to large datasets without requiring strict dependencies.</li>
    <li><strong>Accuracy:</strong> Achieves comparable reconstruction error to EigenGame-\(\mu\) while offering theoretical guarantees.</li>
</ul>
<center>
<img class="rounded" src="./assets/img/pd_experiments1.png" alt="Experimental Results" style="width:720px;">
<img class="rounded" src="./assets/img/pd_experiments2.png" alt="Experimental Results" style="width:720px;">
</center>
    
<br><hr>
<center>
    <h2>Conclusion and Future Work</h2>
</center>
<p>
We present <code>Parallel Deflation</code>, a novel model-parallel PCA framework that removes sequential dependencies and provides provable convergence guarantees. Future work includes:
</p>
<ul>
    <li>Extending the method to other eigenvalue problems.</li>
    <li>Exploring adaptive update strategies for further efficiency.</li>
    <li>Applying the approach to large-scale deep learning models.</li>
</ul>

<br><hr>
<center>
    <h1>Acknowledgements</h1>
    <p>Supported by the Ken Kennedy Institute at Rice University.</p>
</center>
  <br>
  <br>
  <br>
  
